{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8c7eaf",
   "metadata": {},
   "source": [
    "Measure the mobility of characters in books in [litbank](https://github.com/dbamman/litbank/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d8e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e1b91",
   "metadata": {},
   "source": [
    "Let's get all the gutenberg IDs from litbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e76354",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_file = \"../data/gutenberg_metadata/publication_dates.csv\"\n",
    "litbank_books = pd.read_csv (books_file, sep=\"|\")\n",
    "litbank_books = litbank_books.drop(['Unnamed: 0', 'Unnamed: 5'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2972a",
   "metadata": {},
   "source": [
    "Now read the geonames from the file and store it as fast lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247faf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12355198it [12:05, 17022.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7575941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "geonames_file = \"../data/geonames/lat_longs.tsv\"\n",
    "geo_names = pd.read_csv (geonames_file, sep=\"\\t\")\n",
    "geo_names_lookup = defaultdict (list)\n",
    "for i, row in tqdm (geo_names.iterrows ()):\n",
    "    geo_names_lookup[str(row[\"name\"]).lower()].append ((row[\"lat\"], row[\"lon\"]))\n",
    "print (len (geo_names_lookup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b6259",
   "metadata": {},
   "source": [
    "Now we will read all the collocations from all the books and keep only the ones that are GPE tagged for place names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dca1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85466\n"
     ]
    }
   ],
   "source": [
    "collocations = list ()\n",
    "for i, row in litbank_books.iterrows ():\n",
    "    gut_id = row[\"Gutenberg ID\"]\n",
    "    filename = os.path.join (collocations_dir, f\"{gut_id}.collocations\")\n",
    "    if os.path.exists (filename):\n",
    "        collocation_book = pd.read_csv (filename, sep=\"\\t\")\n",
    "        collocations.append (collocation_book)\n",
    "all_collocations = pd.concat (collocations)\n",
    "# Keep only those locations that are tagged as GPE\n",
    "all_collocations = all_collocations[all_collocations[\"locations_cat\"] == \"GPE\"]\n",
    "# Keep only those locations that are present in geo names lookup\n",
    "all_collocations = all_collocations[all_collocations[\"locations_text\"].str.lower().isin (geo_names_lookup)]\n",
    "print (len (all_collocations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c15976",
   "metadata": {},
   "source": [
    "Generate a test set of examples now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee3f7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62704it [2:22:03,  7.36it/s] \n"
     ]
    }
   ],
   "source": [
    "context_window = 100\n",
    "content_dirs = [\n",
    "    \"/mnt/data0/kentkchang/charemotions/corpus/booknlp.1.0.7/gutenberg_fiction_tagged_1_4\",\n",
    "    \"/mnt/data0/kentkchang/charemotions/corpus/booknlp.1.0.7/gutenberg_fiction_tagged_5_9\"\n",
    "]\n",
    "output = list ()\n",
    "for i,row in tqdm (all_collocations.iterrows ()):\n",
    "    book_id = row[\"book_id\"]\n",
    "    start = min(int (row[\"persons_start_token\"]), int (row[\"locations_start_token\"]))\n",
    "    end = max(int (row[\"persons_end_token\"]), int(row[\"locations_end_token\"]))\n",
    "    from_here = start - context_window\n",
    "    till_there = end + context_window\n",
    "    for content_dir in content_dirs:\n",
    "        path = os.path.join (content_dir, f\"{book_id}.tokens\")\n",
    "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "            break\n",
    "    \n",
    "    with open (path) as fin:\n",
    "        context = list ()\n",
    "        for j, line in enumerate (fin):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            parts = line.strip().split (\"\\t\")\n",
    "            token_id = int (parts[3])\n",
    "            if token_id < from_here:\n",
    "                continue\n",
    "            elif token_id >= from_here and token_id <= till_there:\n",
    "                context.append (parts[4])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context = \" \".join (context)\n",
    "        toks = context.split ()\n",
    "        for k in range (context_window + end + 1 - start, len (toks)):\n",
    "            if toks[k] == \".\": # end of sentence\n",
    "                break\n",
    "        toks = toks[0:k+1]\n",
    "    row[f\"context_{context_window}\"] = context\n",
    "    output.append (row)\n",
    "output_df = pd.DataFrame (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e8d4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv (\"../data/experiments/mobility_analysis/testset.tsv\", sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ff890",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "Now let's train our two classifiers: (a) To predict whether the category belongs to the good or bad categories and (b) To predict the actual spatial category.\n",
    "\n",
    "We'll then apply these classifiers on the test set that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3342bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed (96)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b70b5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sublist (sentence_as_tokens, entity_as_tokens):\n",
    "    \"\"\"\n",
    "    For the entity span (decomposed into tokens), find all the start and end\n",
    "    positions within the sentence (decomposed into tokens)\n",
    "    \"\"\"\n",
    "    sublist_positions = list ()\n",
    "    for i, token in enumerate (sentence_as_tokens):\n",
    "        #print (token, entity_as_tokens[0])\n",
    "        if token == entity_as_tokens[0]: # see if the first character matches\n",
    "            if all ([tok == sentence_as_tokens[i+j] if (i+j) < len(sentence_as_tokens) else False for j,tok in enumerate (entity_as_tokens)]):\n",
    "                sublist_positions.append ((i, i+len(entity_as_tokens)))\n",
    "\n",
    "    return sublist_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9fea3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTRelationPrediction (nn.Module):\n",
    "    def __init__ (self, model_name=\"bert-base-cased\", bert_dims=768, n_labels=8):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False, do_basic_tokenize=False)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.n_labels = n_labels\n",
    "        self.fc = nn.Linear (2*bert_dims, self.n_labels)\n",
    "\n",
    "    def forward (self, text, per_entity_span, loc_entity_span, device=\"cpu\"):\n",
    "        # get entity span representations, concatenate and pass it through a \n",
    "        # feedforward network.\n",
    "        token_wordpieces = self.tokenizer.convert_ids_to_tokens (self.tokenizer (text)['input_ids'][1:-1])\n",
    "        per_entity_wordpieces = self.tokenizer.convert_ids_to_tokens(self.tokenizer (per_entity_span)['input_ids'][1:-1])\n",
    "        loc_entity_wordpieces = self.tokenizer.convert_ids_to_tokens(self.tokenizer (loc_entity_span)['input_ids'][1:-1])\n",
    "\n",
    "        # We'll have to change this eventually\n",
    "        per_entity_positions = search_sublist (token_wordpieces, per_entity_wordpieces)\n",
    "        loc_entity_positions = search_sublist (token_wordpieces, loc_entity_wordpieces)\n",
    "\n",
    "        encoded_input = self.tokenizer (text, return_tensors=\"pt\")\n",
    "        encoded_input.to(device)\n",
    "        _, pooled_inputs, sequence_outputs =  self.bert (**encoded_input, output_hidden_states=True, return_dict=False)\n",
    "        last_layer_output = sequence_outputs[-1][0]\n",
    "        per_entity_repr = last_layer_output[per_entity_positions[0][0]: per_entity_positions[0][1],:].mean (dim=0)\n",
    "        loc_entity_repr = last_layer_output[loc_entity_positions[0][0]: loc_entity_positions[0][1],:].mean (dim=0)\n",
    "\n",
    "        input_repr = torch.cat ((per_entity_repr, loc_entity_repr), 0)\n",
    "        output = self.fc (input_repr)\n",
    "        return output\n",
    "\n",
    "    def evaluate (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47d810c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████                                                                                                                                                          | 85/1261 [01:19<18:01,  1.09it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "  7%|███████████                                                                                                                                                          | 85/1261 [01:19<18:22,  1.07it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (517) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14718/3397251740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spatial SuperRelation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#label = train_df[\"Spatial Relation\"].iloc[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertRE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_entity_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_entity_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0my_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccepted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print (y_pred, torch.tensor (y_truth))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14718/1457310697.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, per_entity_span, loc_entity_span, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mencoded_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_outputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlast_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mper_entity_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mper_entity_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mper_entity_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    985\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (517) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "annotations_file = \"/mnt/data0/ssoni/projects/mobility-books/data/annotations/final_annotations/final_annotations.v1.tsv\"\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "num_epochs = 10\n",
    "context_field = \"context_100\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "all_labels = [ \"NO RELATIONSHIP ASSERTED\",\n",
    "               \"TOWARD(got there)\",\n",
    "               \"FROM\",\n",
    "               \"NEAR\",\n",
    "               \"IN\",\n",
    "               \"NEGATIVE ASSERTION\",\n",
    "               \"THROUGH\",\n",
    "               \"TOWARD (uncertain got there)\",\n",
    "               \"BAD LOC\",\n",
    "               \"BAD PER\",\n",
    "               \"UNCERTAIN ASSERTION\"]\n",
    "\n",
    "bad_labels = [ \"BAD LOC\",\n",
    "               \"BAD PER\",\n",
    "               \"UNCERTAIN ASSERTION\"]\n",
    "\n",
    "label_names = {0:\"GOOD\", 1:\"BAD\"}\n",
    "accepted_labels = [\"GOOD\", \"BAD\"]\n",
    "\n",
    "df = pd.read_csv (annotations_file, sep=\"\\t\")\n",
    "df = df[df[\"Spatial Relation\"] != \"\"]\n",
    "df = df[df[\"Spatial Relation\"].isin (all_labels)]\n",
    "df[\"Spatial SuperRelation\"] = df[\"Spatial Relation\"].isin (bad_labels)\n",
    "#train_df, test_df = train_test_split(df, test_size=0.2, random_state=96)\n",
    "\n",
    "bertRE = BERTRelationPrediction (model_name=pretrained_model_name, bert_dims=768, n_labels=2)\n",
    "bertRE.to(device)\n",
    "optimizer = torch.optim.Adam(bertRE.parameters(), lr=1e-5)\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (f\"Epoch: {epoch}\")\n",
    "    bertRE.train()\n",
    "    # Train\n",
    "    for i in tqdm (range (len (df))):\n",
    "        # get the extracted quantities\n",
    "        text = df[context_field].iloc[i]\n",
    "        per_entity_span = df[\"persons_text\"].iloc[i]\n",
    "        loc_entity_span = df[\"locations_text\"].iloc[i]\n",
    "        label = label_names[int (df[\"Spatial SuperRelation\"].iloc[i])]\n",
    "        #label = train_df[\"Spatial Relation\"].iloc[i] \n",
    "        y_pred = bertRE.forward (text, per_entity_span, loc_entity_span, device=device)\n",
    "        y_truth = accepted_labels.index (label)\n",
    "        #print (y_pred, torch.tensor (y_truth))\n",
    "        loss = cross_entropy (y_pred.unsqueeze (0), torch.tensor ([y_truth]).to(device))\n",
    "        optimizer.zero_grad ()\n",
    "        loss.backward ()\n",
    "        optimizer.step ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "   groundtruth, predictions = list (), list ()\n",
    "    bertRE.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm (range (len (test_df))):\n",
    "            # get the extracted quantities\n",
    "            text = test_df[args.context].iloc[i]\n",
    "            per_entity_span = test_df[\"persons_text\"].iloc[i]\n",
    "            loc_entity_span = test_df[\"locations_text\"].iloc[i]\n",
    "            #label = test_df[\"Spatial Relation\"].iloc[i]\n",
    "            label = label_names[int (test_df[\"Spatial SuperRelation\"].iloc[i])]\n",
    "            y_truth = accepted_labels.index (label)\n",
    "            y_pred = bertRE.forward (text, per_entity_span, loc_entity_span, device=device)\n",
    "            groundtruth.append (y_truth)\n",
    "            predictions.append (torch.argmax (torch.nn.functional.softmax (y_pred)).item())\n",
    "\n",
    "    print (classification_report (groundtruth, predictions))\n",
    "    print (classification_report (groundtruth, [4]*len(predictions))) #baseline\n",
    "\n",
    "test_df[\"ground_truth\"] = groundtruth\n",
    "test_df[\"predictions\"] = predictions\n",
    "\n",
    "test_df.to_csv (args.output_filename, sep=\"\\t\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
